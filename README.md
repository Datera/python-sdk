# Datera Python SDK


## Introduction

This is Python SDK version 0.1 for the **Datera** Fabric Services API.
Download and use of this package implicitly accepts the terms in COPYING

Users of this package are assumed to have familiarity with the **Datera** API.
Details around the API itself are not necessarily covered through this SDK.


## Installation

To install:
```bash
    apt-get install python-virtualenv (or yum install python-virtualenv for CentOS)
    virtualenv sdk
    source sdk/bin/activate
    git clone https://github.com/Datera/python-sdk.git
    cd python-sdk
    pip install -r requirements.txt
    python setup.py install
```

## Datera Universal Config

The Datera Universal Config (DUC) is a config that can be specified in a
number of ways:

* JSON file with any of the following names:
    [.datera-config, datera-config, .datera-config.json, dateraconfig.json]
* The JSON file has the following configuration:
```json
     {"mgmt_ip": "1.1.1.1",
      "username": "admin",
      "password": "password",
      "tenant": "/root",
      "api_version": "2.2"}
```
* The file can be in any of the following places.  This is also the lookup
  order for config files:
    current directory --> home directory --> home/config directory --> /etc/datera
* If no datera config file is found and a cinder.conf file is present, the
  config parser will try and pull connection credentials from the
  cinder.conf
* Instead of a JSON file, environment variables can be used.
    - DAT_MGMT
    - DAT_USER
    - DAT_PASS
    - DAT_TENANT
    - DAT_API
* Most tools built to use the Datera Universal Config will also allow
  for providing/overriding any of the config values via command line flags.
    - --hostname
    - --username
    - --password
    - --tenant
    - --api-version

## Developing with Datera Universal Config

To use DUC in a new python tool is very simple just add the following to
your python script:

```python
from dfs_sdk import scaffold

parser = scaffold.get_argparser()
parser.add_argument('my-new-arg')
args = parser.parse_args()
```

If you want to use subparsers, or customize the help outptu of your parser
then use the following

```python
import argparse
from dfs_sdk import scaffold

top_parser = scaffold.get_argparser(add_help=False)
new_parser = argparse.ArgumentParser(parents=[top_parser])
new_parser.add_argument('my-new-arg')
args = new_parser.parse_args()
```

Inside a script the config can be recieved by calling
```python
from dfs_sdk import scaffold

scaffold.get_argparser()
config = scaffold.get_config()
```
NOTE: You MUST call ``scaffold.get_argparser()`` before calling
``scaffold.get_config()``.  This may change in the future

## Logging

To set custom logging.json file
```bash
    export DSDK_LOG_CFG=your/log/location.json
```
Or the value can be set to a debug, info or error
```bash
    export DSDK_LOG_CFG=info
```

To set logging to stdout.  The value can be any logging level supported by
the python logging module (eg: debug, info, etc)
```bash
    export DSDK_LOG_STDOUT=debug
```

The debug logs generated by the python-sdk are quite large, and are on a
rotating file handler (provided that a custom logging.json file is not provided)

## Managed Objects

Datera provides an application-driven storage management model, whose goal is to closely align storage
with a corresponding application's requirements.

The main storage objects are defined and differentiated as follows:

### Application Instance (AppInstance)
    -    Corresponds to an application, service, etc.
    -    Contains Zero or more Storage Instances

### Storage Instance
    -    Corresponds to one set of storage requirements for a given AppInstance
    -    ACL Policies, including IQN Initiators
    -    Target IQN
    -    Contains Zero or more Volumes

### Volumes
    -    Corresponds to a single allocated storage object
    -    Size (default unit is GB)
    -    Replication Factor
    -    Performance Policies (QoS for Bandwidth and IOPS)
    -    Protection Policies (Snapshot scheduling)

Another way of viewing the managed object hierarchy is as follows:

    app_instances:
        - storage_instances:                 (1 or more per app_instance)
            + acl_policy                     (1 or more host initiators )
            + iqn                            (target IQN)
            + ips                            (target IPs)
            + volumes:                       (1 or more per storage_instance)
                * name
                * size
                * replication
                * performance_policy         (i.e. QoS)
                * protection_policy          (i.e. Snapshot schedules)


## Endpoints

HTTP operations on URL endpoints is the only way to interact with the set of managed objects.
URL's have the format:
```bash
      http://192.168.42.13:7717/v2/<object_class>/[<instance>]/...
```
where **7717** is the port used to access the API, and "v2" corresponds to an API version control.

Briefly, the REST API supports 4 operations/methods **create (POST), modify (PUT), list (GET), delete (DELETE)**.
Any input payload is in JSON format;  any return payload is in JSON format.
Login session keys are required within the "header" of any HTTP request.
Sessions keys have a 15 minute lifetime.

For a full reference documentation of the REST API, please review the Datera REST API Guide.

This Python SDK serves as a wrapper around the raw HTTP layer.

## Using this SDK

The Datera module is named **dfs_sdk**, and the main entry point is called __DateraApi__.
Obtaining an object handle can be done as follows:
```python
        from dfs_sdk import DateraApi
        [...]
        api = DateraApi(username=user, password=password, hostname=ipaddr)
```


## Common Objects, Examples and  Use Cases

Please see the **utils** directory for programming examples that cover the following:

Common methods for all objects include **create(), set(), delete(), list()**

+ To create an app_instance with name **FOO**:
```python
        ai = api.app_instances.create(name="FOO")
```
+ Looping through objects can be done via **list()**:
```python
        for ai in api.app_instances.list():
            print "AppInstance: ", ai
```
+ To set a given **app_instance** into an _offline_ state:
```python
        ai.set(admin_state="offline")
```
+ To delete a given app_instance:
```python
        ai.delete()
```
## (DEPRECATED) 'dhutil' : Datera Host Utility

### NOTE: This utility has been deprecated by the Datera Bare-Metal Provisioner (dbmp).  http://github.com/Datera/dbmp

The 'dhutil' host-utility is provided along with this SDK.
'dhutil' can be used as both a reference example for using the SDK,
as well as providing some common host-side utility.  For example, a given storage/application lifecycle might looks like this:

+ Create 5 app_instances named 'mongodev', each with a single 10G volume,
and to perform the host-side iscsi scan and login:
```bash
          dhutil --basename mongodev --count 5 --size 10
```
+ View the multipath mapping to the host:
```bash
         dhutil --mpmap
```
+ Create **xfs** filesystems for the 'mongodb' volumes,  mount them at '/mnt' and change the permissions to 'mongodb:mongodb':
```bash
          dhutil --basename mongodev --mkfs --dirprefix /mnt --chown mongodb:mongodb
```
+ Completely teardown (unmount, remove directory, iscsi logout, delete app_instances):
```bash
          dhutil --basename mongodev --cleanall
```
Note that steps 1 and 3 could be combined as follows:
```bash
          dhutil --basename mongodev --count 5 --size 10 --mkfs --dirprefix /mnt --chown mongodb:mongodb
```
Or a corresponding "app_template" could be used, if available:
```bash
          dhutil --basename mongodev --count 5 --template mongodb ...
```
#### Caveats
- **dhutil** presumes a 'singleton' model, whereby an app_instance
    is created with a single storage_instance with a single volume.
    Extending the functionality is left as an exercise for the reader
and is strongly encouraged!


## Reporting Problems

For problems and feedback, please email "support@datera.io"
